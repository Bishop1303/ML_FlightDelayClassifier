{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_RandomForestEnsamble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPUnZx2uTImsLIbHz2/oUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bishop1303/ML_PySpark/blob/dev/ML_RandomForestEnsamble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75AmctNtDHfK"
      },
      "source": [
        "#Carica il drive con i dati:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Getting the softwares:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "# To use spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "# SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilenx7jZDW22"
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer, OneHotEncoder, VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import round\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "### DATA PREPARATION ###\n",
        "\n",
        "# Read raw data to pyspark\n",
        "flights = spark.read.csv(\"/content/drive/My Drive/flights.csv\", inferSchema=True, header=True, mode='FAILFAST')\n",
        "#flights.show()\n",
        "#flights.printSchema()\n",
        "\n",
        "### DATA PREPARATION ###\n",
        "\n",
        "# Remove the 'flight' column\n",
        "flights_drop_column = flights.drop('flight')\n",
        "\n",
        "# Remove records with missing 'delay' values or NA values\n",
        "flights_none_missing = flights_drop_column.filter((flights_drop_column.delay.isNotNull()) & \\\n",
        "                                                  (flights_drop_column.delay != 'NA'))\n",
        "\n",
        "# Change delay type to int\n",
        "flights_none_missing = flights_none_missing.withColumn('delay', flights_none_missing['delay'].cast('int'))\n",
        "\n",
        "# Check on dataframe\n",
        "#print('The Schema is: ')\n",
        "#flights_none_missing.printSchema()\n",
        "#print('=====================================================')\n",
        "#print('Informative rows after dropping malformed: ',flights_none_missing.count())\n",
        "\n",
        "\n",
        "# Conversion: 'mile' to 'km' and drop 'mile' column\n",
        "flights_km = flights_none_missing.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')\n",
        "\n",
        "# Creating 'label' column indicating whether flight delayed (1) or not (0)\n",
        "flights_ready = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
        "\n",
        "# Check records\n",
        "#flights_ready.show(5)\n",
        "#flights_ready.printSchema()\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpX3PBQgH0zg"
      },
      "source": [
        "### DATA PREPARATION FOR ML ###\n",
        "\n",
        "# Create buckets at 3 hour intervals through the day\n",
        "#splits=[0,3,6,9,12,15,18,21,24]\n",
        "#buckets = Bucketizer(splits=splits, inputCol='depart', outputCol='depart_bucket')\n",
        "\n",
        "# Bucket the departure times\n",
        "#bucketed = buckets.transform(flights_ready)\n",
        "#bucketed.select('depart','depart_bucket').show(5)\n",
        "\n",
        "# Create a one-hot encoder\n",
        "#onehot = OneHotEncoder(inputCol=buckets.getOutputCol(), outputCol='depart_dummy')\n",
        "\n",
        "# One-hot encode the bucketed departure times\n",
        "#flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
        "#flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)\n",
        "\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=['mon','depart','duration'], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(bucketed)\n",
        "\n",
        "# Check the resulting column\n",
        "flights_assembled = flights_assembled.select('mon','depart','duration','features','label')\n",
        "\n",
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=7)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_dcp2hsEGJ-",
        "outputId": "154b58a0-4cf4-4d04-da04-255a9b43123f"
      },
      "source": [
        "\n",
        "# Create model objects and train on training data\n",
        "tree = DecisionTreeClassifier().fit(flights_train)\n",
        "gbt = GBTClassifier().fit(flights_train)\n",
        "\n",
        "# Compare AUC on testing data\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "evaluator.evaluate(tree.transform(flights_test))\n",
        "evaluator.evaluate(gbt.transform(flights_test))\n",
        "\n",
        "# Find the number of trees and the relative importance of features\n",
        "print('The number of trees is: ',gbt.getNumTrees)\n",
        "print('The importance of the features is: ',gbt.featureImportances)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of trees is:  20\n",
            "The importance of the features is:  (3,[0,1,2],[0.3530586081199667,0.314137501885979,0.33280388999405425])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stza3DZCZYX3"
      },
      "source": [
        "\n",
        "\n",
        "forest = RandomForestClassifier()\n",
        "\n",
        "# Create a parameter grid\n",
        "params = ParamGridBuilder() \\\n",
        "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
        "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
        "            .build()\n",
        "\n",
        "# Create a binary classification evaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "# Create a cross-validator\n",
        "cv = CrossValidator(estimator=forest,\n",
        "                    estimatorParamMaps=params, \n",
        "                    evaluator=evaluator, \n",
        "                    numFolds=5\n",
        "                    ).fit(flights_train)\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK32rqiobC42"
      },
      "source": [
        "# Evaluating Random Forest\n",
        "\n",
        "* cv - a cross-validator which has already been fit to the training data\n",
        "\n",
        "* evaluator â€” a BinaryClassificationEvaluator object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mu1KrlnbBoC",
        "outputId": "ea2e9811-c50c-439e-d730-716ef8c718bd"
      },
      "source": [
        "# Average AUC for each parameter combination in grid\n",
        "avg_auc = cv.avgMetrics\n",
        "print(avg_auc)\n",
        "\n",
        "# Average AUC for the best model\n",
        "print(max(cv.avgMetrics)) \n",
        "\n",
        "# What's the optimal parameter value for maxDepth?\n",
        "print(cv.bestModel.explainParam('maxDepth'))\n",
        "# What's the optimal parameter value for featureSubsetStrategy?\n",
        "print(cv.bestModel.explainParam('featureSubsetStrategy'))\n",
        "\n",
        "# AUC for best model on testing data\n",
        "print('AUC for best model on testing data: ','{:,.3f}'.format(evaluator.evaluate(cv.transform(flights_test))))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6237401501928062, 0.6627639948562409, 0.671435148541434, 0.6467624333232634, 0.6649439350208881, 0.6760481739876215, 0.6402999535732408, 0.6648675676141262, 0.6720475336770333, 0.6402999535732408, 0.6648675676141262, 0.6720475336770333]\n",
            "0.6760481739876215\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 10)\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto, current: onethird)\n",
            "AUC for best model on testing data:  0.681\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}