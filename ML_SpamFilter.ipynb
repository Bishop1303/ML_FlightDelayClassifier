{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_SpamFilter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYShGz+iJeDX9XyuLydy+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bishop1303/ML_PySpark/blob/dev/ML_SpamFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oktuXR8cWy2b"
      },
      "source": [
        "#Carica il drive con i dati:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDXEtU9MWTD9"
      },
      "source": [
        "# Getting the softwares:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "# To use spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "# SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPCoE6YKXBbZ"
      },
      "source": [
        "# From Unstructured text data to Structured text data\n",
        "\n",
        "The objective is to build a spam filter using ML. The dataset *sms.csv* contains sms already classfied as spam (1) or not (0).\n",
        "\n",
        "```\n",
        "+---+--------------------+-----+\n",
        "| id|                text|label|\n",
        "+---+--------------------+-----+\n",
        "|  1|Sorry, I'll call ...|    0|\n",
        "|  2|Dont worry. I gue...|    0|\n",
        "|  3|Call FREEPHONE 08...|    1|\n",
        "        ...\n",
        "```\n",
        "To be able to use the text data for ML *few* actions are needed:\n",
        "\n",
        "* remove punctuation and numbers\n",
        "* tokenize (split into individual words)\n",
        "* remove stop words (i.e. those words that do not provide any useful information to decide in which category a text should be classified).\n",
        "* apply the hashing trick\n",
        "* convert to TF-IDF representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63jOMhU2Y3Uy"
      },
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# Reading data\n",
        "sms = spark.read.csv(\"/content/drive/My Drive/sms.csv\", inferSchema=True, header=False, sep=';').toDF(\"id\",\"text\",\"label\")\n",
        "\n",
        "# Check DF\n",
        "#sms.show()\n",
        "#sms.printSchema()\n",
        "\n",
        "# Remove punctuation\n",
        "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
        "\n",
        "# Remove numbers\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
        "\n",
        "# Merge multiple spaces \n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
        "\n",
        "# Split the text into words\n",
        "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
        "\n",
        "wrangled.show(4, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJk_ysyAeCm_"
      },
      "source": [
        "# Stop words and hashing\n",
        "\n",
        "Remove stop words, apply the **hashing trick** and converting the results into a **TF-IDF Matrix**.\n",
        "\n",
        "The **hashing trick** provides a fast and space-efficient way to map a very large (possibly infinite) set of items (in this case, all words contained in the SMS messages) onto a smaller, finite number of values.  \n",
        "\n",
        "NB: *HashingTF* argument *numFeatures* tells the lenght of the hash code assigned to a word in the doc. Even with a numFeatures big as vocabulary size is possible to have duplicate \"unique\" hash for different words that's bad ofc..\n",
        "\n",
        "The **TF-IDF Matrix** reflects how important a word is to each document. It takes into account both:\n",
        "\n",
        "\n",
        "*   The frequency of the word within each document.\n",
        "\n",
        "*   The frequency of the word across all of the documents in the collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ebnJMh3d_Vx"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# Remove stop words.\n",
        "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Apply the hashing trick\n",
        "wrangled = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024)\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Convert hashed symbols to TF-IDF\n",
        "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
        "      .fit(wrangled).transform(wrangled)\n",
        "      \n",
        "tf_idf.select('terms', 'features').show(4, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miLl6nc1mSNI"
      },
      "source": [
        "# Training the spam classifier\n",
        "\n",
        "The SMS data have now been prepared for building a classifier.  \n",
        "\n",
        "Next steps are:\n",
        "1.   Split the **TF-IDF** data into *training* and *testing sets*.\n",
        "2.   Use the training data to fit a Logistic Regression model.\n",
        "3.   Evaluate the performance of that model on the testing data.\n",
        "\n",
        "NB. Regularization (*regParam*, default 0.0) is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients donâ€™t take extreme values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At6huLqtm5Xw"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Selecting right columns of sms manipulated data\n",
        "sms_ml_ready = tf_idf.select('label', 'features')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "sms_train, sms_test = sms_ml_ready.randomSplit([0.8,0.2], seed=13)\n",
        "\n",
        "# Fit a Logistic Regression model to the training data\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "prediction = logistic.transform(sms_test)\n",
        "\n",
        "# Confusion matrix, comparing predictions to known labels\n",
        "prediction.groupBy('label', 'prediction').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IzM3_Lcp4il"
      },
      "source": [
        "# One-Hot Encoding\n",
        "\n",
        "\n"
      ]
    }
  ]
}